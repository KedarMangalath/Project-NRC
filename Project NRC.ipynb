{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c7a1486",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to clean and preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8777538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate NRC sentiment scores\n",
    "def calculate_emotion_scores(text, nrc_lexicon):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    emotion_scores = {emotion: 0 for emotion in nrc_lexicon['word'].keys()}\n",
    "    for token in tokens:\n",
    "        if token in nrc_lexicon:\n",
    "            for emotion in nrc_lexicon[token]:\n",
    "                emotion_scores[emotion] += nrc_lexicon[token][emotion]\n",
    "    return np.array(list(emotion_scores.values()))\n",
    "\n",
    "nrc_lexicon = {}\n",
    "with open(r\"C:\\Users\\manga\\OneDrive\\Desktop\\NRC-Emotion-Lexicon\\NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", 'r') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        word, emotion, score = row\n",
    "        if word not in nrc_lexicon:\n",
    "            nrc_lexicon[word] = {}\n",
    "        nrc_lexicon[word][emotion] = int(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31ef584d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 13s 99ms/step - loss: 1.3492 - accuracy: 0.3326 - val_loss: 1.2358 - val_accuracy: 0.4464\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 7s 86ms/step - loss: 0.9988 - accuracy: 0.6194 - val_loss: 0.9427 - val_accuracy: 0.6955\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 7s 86ms/step - loss: 0.5507 - accuracy: 0.8493 - val_loss: 0.7698 - val_accuracy: 0.7439\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 7s 83ms/step - loss: 0.2924 - accuracy: 0.9350 - val_loss: 0.6611 - val_accuracy: 0.7924\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 8s 94ms/step - loss: 0.1502 - accuracy: 0.9731 - val_loss: 0.7402 - val_accuracy: 0.7578\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 7s 86ms/step - loss: 0.0902 - accuracy: 0.9800 - val_loss: 1.0261 - val_accuracy: 0.7197\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 7s 84ms/step - loss: 0.0714 - accuracy: 0.9819 - val_loss: 0.7670 - val_accuracy: 0.7820\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 7s 84ms/step - loss: 0.0521 - accuracy: 0.9858 - val_loss: 0.7925 - val_accuracy: 0.7820\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 7s 84ms/step - loss: 0.0439 - accuracy: 0.9862 - val_loss: 0.7240 - val_accuracy: 0.7958\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 8s 92ms/step - loss: 0.0416 - accuracy: 0.9846 - val_loss: 0.8282 - val_accuracy: 0.7612\n",
      "23/23 [==============================] - 2s 38ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.85      0.70      0.77       180\n",
      "        fear       0.77      0.80      0.78       207\n",
      "         joy       0.93      0.75      0.83       170\n",
      "     sadness       0.60      0.81      0.69       166\n",
      "\n",
      "    accuracy                           0.76       723\n",
      "   macro avg       0.79      0.76      0.77       723\n",
      "weighted avg       0.79      0.76      0.77       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset (assuming the format is ID, Tweet, Emotion, Intensity)\n",
    "def load_data(file_path, nrc_lexicon):\n",
    "    data = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    data.columns = ['ID', 'Tweet', 'Emotion', 'Intensity']\n",
    "    data['Processed_Tweet'] = data['Tweet'].apply(preprocess_text)\n",
    "    data['Emotion_Scores'] = data['Processed_Tweet'].apply(lambda x: calculate_emotion_scores(x, nrc_lexicon))\n",
    "    return data\n",
    "\n",
    "# Load and preprocess the data\n",
    "anger_data = load_data(r\"C:\\Users\\manga\\OneDrive\\Desktop\\Project\\Train\\anger_training.txt\", nrc_lexicon)\n",
    "fear_data = load_data(r\"C:\\Users\\manga\\OneDrive\\Desktop\\Project\\Train\\fear_training.txt\", nrc_lexicon)\n",
    "joy_data = load_data(r\"C:\\Users\\manga\\OneDrive\\Desktop\\Project\\Train\\joy_training.txt\", nrc_lexicon)\n",
    "sadness_data = load_data(r\"C:\\Users\\manga\\OneDrive\\Desktop\\Project\\Train\\sadness_training.txt\", nrc_lexicon)\n",
    "\n",
    "# Combine the data from all emotions\n",
    "combined_data = pd.concat([anger_data,fear_data, joy_data, sadness_data])  # Combine all the loaded data frames\n",
    "combined_data = combined_data.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle the combined data\n",
    "\n",
    "# Tokenization and sequence padding\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(combined_data['Processed_Tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(combined_data['Processed_Tweet'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Encode emotion labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(combined_data['Emotion'])\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# ... [previous code for preprocessing and preparing your data] ...\n",
    "\n",
    "# Keep a copy of the indices of the combined_data before splitting\n",
    "indices = combined_data.index\n",
    "\n",
    "# Split the data into training and testing sets for both models\n",
    "# Make sure to also split the indices to keep track of which rows go into the test set\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
    "    padded_sequences, categorical_labels, indices, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ... [insert the rest of your model training and evaluation code here] ...\n",
    "\n",
    "# Split the data into training and testing sets for both models\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, categorical_labels, test_size=0.2, random_state=42)\n",
    "emotion_scores_train, emotion_scores_test = train_test_split(combined_data['Emotion_Scores'].tolist(), test_size=0.2, random_state=42)\n",
    "intensity_train, intensity_test = train_test_split(combined_data['Intensity'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Emotion Classification Model\n",
    "# Define the architecture\n",
    "input_layer = Input(shape=(100,))\n",
    "embedding_layer = Embedding(10000, 64)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(64))(embedding_layer)\n",
    "output_layer = Dense(categorical_labels.shape[1], activation='softmax')(lstm_layer)\n",
    "classification_model = Model(input_layer, output_layer)\n",
    "\n",
    "# Compile the model\n",
    "classification_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "classification_model.fit(X_train, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the classification model\n",
    "y_pred_class = classification_model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_class, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e542fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 1s 3ms/step - loss: 0.1552 - mean_squared_error: 0.1552 - val_loss: 0.0633 - val_mean_squared_error: 0.0633\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0527 - mean_squared_error: 0.0527 - val_loss: 0.0383 - val_mean_squared_error: 0.0383\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0417 - mean_squared_error: 0.0417 - val_loss: 0.0351 - val_mean_squared_error: 0.0351\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0385 - mean_squared_error: 0.0385 - val_loss: 0.0328 - val_mean_squared_error: 0.0328\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0367 - mean_squared_error: 0.0367 - val_loss: 0.0320 - val_mean_squared_error: 0.0320\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0355 - mean_squared_error: 0.0355 - val_loss: 0.0330 - val_mean_squared_error: 0.0330\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0343 - mean_squared_error: 0.0343 - val_loss: 0.0318 - val_mean_squared_error: 0.0318\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0335 - val_mean_squared_error: 0.0335\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0340 - mean_squared_error: 0.0340 - val_loss: 0.0319 - val_mean_squared_error: 0.0319\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.0329 - mean_squared_error: 0.0329 - val_loss: 0.0322 - val_mean_squared_error: 0.0322\n",
      "23/23 [==============================] - 1s 31ms/step\n",
      "23/23 [==============================] - 0s 928us/step\n",
      "                                      Original Tweet Actual Emotion  \\\n",
      "0  The point of living, and being an optimist, is...            joy   \n",
      "1  Just want to be a dad so i can play kick the b...           fear   \n",
      "2  @AHSFX thanks Ryan &amp; Brad for scary the sh...           fear   \n",
      "3  @AdsByFlaherty And you're cheerfully defending...            joy   \n",
      "4  PM #SheikhHasina in @UN speech terms #terroris...           fear   \n",
      "5  @carysmithwriter @Maria_Savva @RealRockAndRoll...            joy   \n",
      "6  @AMB4JC @drtonyevans It's our job, the job of ...        sadness   \n",
      "7  @delon03 can you at least just walk past her a...            joy   \n",
      "8  That last minute was like watching a horror sh...           fear   \n",
      "9  @morgannbroom yes â¤ï¸â¤ï¸ &amp; cheering ...            joy   \n",
      "\n",
      "  Predicted Emotion  Actual Intensity  Predicted Intensity  \n",
      "0               joy             0.407             0.395111  \n",
      "1              fear             0.333             0.467277  \n",
      "2              fear             0.875             0.626041  \n",
      "3              fear             0.104             0.480526  \n",
      "4              fear             0.554             0.508342  \n",
      "5               joy             0.479             0.634628  \n",
      "6           sadness             0.214             0.396546  \n",
      "7               joy             0.417             0.517768  \n",
      "8               joy             0.500             0.473750  \n",
      "9               joy             0.780             0.479114  \n",
      "23/23 [==============================] - 0s 871us/step\n",
      "MSE: 0.03404001512583824\n",
      "R2: 0.04773108192096387\n",
      "MAE: 0.14994610394283944\n",
      "Pearson Correlation: 0.2586688558683227\n"
     ]
    }
   ],
   "source": [
    "# Emotion Intensity Regression Model\n",
    "# Define the architecture\n",
    "regression_input_layer = Input(shape=(len(emotion_scores_train[0]),))\n",
    "regression_dense_layer = Dense(64, activation='relu')(regression_input_layer)\n",
    "regression_output_layer = Dense(1)(regression_dense_layer)\n",
    "regression_model = Model(regression_input_layer, regression_output_layer)\n",
    "\n",
    "# Compile the model\n",
    "regression_model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the regression model\n",
    "regression_model.fit(np.array(emotion_scores_train), intensity_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Predict the emotion and its intensity on the test set\n",
    "y_pred_class = classification_model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_class, axis=1)\n",
    "predicted_intensity = regression_model.predict(np.array(emotion_scores_test)).flatten()\n",
    "\n",
    "# Map the predicted emotion labels back to the original string labels\n",
    "predicted_emotions = label_encoder.inverse_transform(y_pred_labels)\n",
    "\n",
    "# Combine the original tweets with the actual and predicted intensities for display\n",
    "results = pd.DataFrame({\n",
    "    'Original Tweet': combined_data.loc[indices_test, 'Tweet'].values,\n",
    "    'Actual Emotion': combined_data.loc[indices_test, 'Emotion'].values,\n",
    "    'Predicted Emotion': predicted_emotions,\n",
    "    'Actual Intensity': combined_data.loc[indices_test, 'Intensity'].values,\n",
    "    'Predicted Intensity': predicted_intensity\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results.head(10))\n",
    "\n",
    "# Evaluate the regression model\n",
    "predicted_intensity = regression_model.predict(np.array(emotion_scores_test))\n",
    "mse = mean_squared_error(intensity_test, predicted_intensity)\n",
    "r2 = r2_score(intensity_test, predicted_intensity)\n",
    "mae = mean_absolute_error(intensity_test, predicted_intensity)\n",
    "pearson_corr, _ = pearsonr(intensity_test, predicted_intensity.flatten())\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R2: {r2}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'Pearson Correlation: {pearson_corr}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a821459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding_8 (Embedding)     (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 128)              66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 706,564\n",
      "Trainable params: 706,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 64)                704       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 769\n",
      "Trainable params: 769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the models\n",
    "classification_model.summary()\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7bf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
